{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:43:50.732118Z",
     "iopub.execute_input": "2023-07-08T13:43:50.732543Z",
     "iopub.status.idle": "2023-07-08T13:43:50.744606Z",
     "shell.execute_reply.started": "2023-07-08T13:43:50.732516Z",
     "shell.execute_reply": "2023-07-08T13:43:50.743663Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('/kaggle/input/new-data-lun-csv/new_data_lun.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:43:50.746536Z",
     "iopub.execute_input": "2023-07-08T13:43:50.747148Z",
     "iopub.status.idle": "2023-07-08T13:43:50.887046Z",
     "shell.execute_reply.started": "2023-07-08T13:43:50.747116Z",
     "shell.execute_reply": "2023-07-08T13:43:50.886082Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "same = train_data[train_data[\"is_same\"]==1][:10000]\n",
    "different = train_data[train_data[\"is_same\"]==0][:12000]\n",
    "new_train_data = pd.concat([same, different])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:43:50.888710Z",
     "iopub.execute_input": "2023-07-08T13:43:50.889082Z",
     "iopub.status.idle": "2023-07-08T13:43:50.906468Z",
     "shell.execute_reply.started": "2023-07-08T13:43:50.889050Z",
     "shell.execute_reply": "2023-07-08T13:43:50.905650Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "new_train_data[\"image_url3\"] = list(new_train_data[\"image_url1\"].values)[::-1]\n",
    "new_train_data = new_train_data[new_train_data[\"is_same\"]==1]\n",
    "new_train_data.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:43:50.909010Z",
     "iopub.execute_input": "2023-07-08T13:43:50.909335Z",
     "iopub.status.idle": "2023-07-08T13:43:50.932640Z",
     "shell.execute_reply.started": "2023-07-08T13:43:50.909305Z",
     "shell.execute_reply": "2023-07-08T13:43:50.931433Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "       image_url1     image_url2  is_same     image_url3\n1   965225293.jpg  965564035.jpg        1  909399908.jpg\n3   917878082.jpg  921610429.jpg        1  926015250.jpg\n4   941374542.jpg  941588763.jpg        1  938000122.jpg\n8   925692435.jpg  925916250.jpg        1  930156741.jpg\n13  945545057.jpg  945547680.jpg        1  892382884.jpg",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_url1</th>\n      <th>image_url2</th>\n      <th>is_same</th>\n      <th>image_url3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>965225293.jpg</td>\n      <td>965564035.jpg</td>\n      <td>1</td>\n      <td>909399908.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>917878082.jpg</td>\n      <td>921610429.jpg</td>\n      <td>1</td>\n      <td>926015250.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>941374542.jpg</td>\n      <td>941588763.jpg</td>\n      <td>1</td>\n      <td>938000122.jpg</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>925692435.jpg</td>\n      <td>925916250.jpg</td>\n      <td>1</td>\n      <td>930156741.jpg</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>945545057.jpg</td>\n      <td>945547680.jpg</td>\n      <td>1</td>\n      <td>892382884.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from six import BytesIO\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:43:50.934547Z",
     "iopub.execute_input": "2023-07-08T13:43:50.934921Z",
     "iopub.status.idle": "2023-07-08T13:44:00.020020Z",
     "shell.execute_reply.started": "2023-07-08T13:43:50.934887Z",
     "shell.execute_reply": "2023-07-08T13:44:00.019122Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = new_train_data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:00.021625Z",
     "iopub.execute_input": "2023-07-08T13:44:00.022417Z",
     "iopub.status.idle": "2023-07-08T13:44:00.028726Z",
     "shell.execute_reply.started": "2023-07-08T13:44:00.022366Z",
     "shell.execute_reply": "2023-07-08T13:44:00.027127Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dir_img_1 = '/kaggle/input/traindataset/LUN_DataSet/train_url1'\n",
    "dir_img_2 = '/kaggle/input/traindataset/LUN_DataSet/train_url2'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:00.031223Z",
     "iopub.execute_input": "2023-07-08T13:44:00.031948Z",
     "iopub.status.idle": "2023-07-08T13:44:00.041886Z",
     "shell.execute_reply.started": "2023-07-08T13:44:00.031913Z",
     "shell.execute_reply": "2023-07-08T13:44:00.040804Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import Input, Sequential, Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import applications, layers, losses, optimizers, metrics\n",
    "\n",
    "mobile_net = VGG16(\n",
    "    weights=\"imagenet\", input_shape=(224,224, 3), include_top=False\n",
    ")\n",
    "\n",
    "flatten = layers.Flatten()(mobile_net.output)\n",
    "x1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x2 = layers.Dense(256, activation=\"relu\")(x1)\n",
    "x2 = layers.BatchNormalization()(x2)\n",
    "output = layers.Dense(256)(x2)\n",
    "\n",
    "embedding = Model(mobile_net.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in mobile_net.layers:\n",
    "    if layer.name == \"block5_conv1\":\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:00.043389Z",
     "iopub.execute_input": "2023-07-08T13:44:00.043753Z",
     "iopub.status.idle": "2023-07-08T13:44:05.526403Z",
     "shell.execute_reply.started": "2023-07-08T13:44:00.043706Z",
     "shell.execute_reply": "2023-07-08T13:44:05.525510Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embedding.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.531388Z",
     "iopub.execute_input": "2023-07-08T13:44:05.531688Z",
     "iopub.status.idle": "2023-07-08T13:44:05.582620Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.531665Z",
     "shell.execute_reply": "2023-07-08T13:44:05.581911Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"Embedding\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n dense (Dense)               (None, 512)               12845568  \n                                                                 \n batch_normalization (BatchN  (None, 512)              2048      \n ormalization)                                                   \n                                                                 \n dense_1 (Dense)             (None, 256)               131328    \n                                                                 \n batch_normalization_1 (Batc  (None, 256)              1024      \n hNormalization)                                                 \n                                                                 \n dense_2 (Dense)             (None, 256)               65792     \n                                                                 \n=================================================================\nTotal params: 27,760,448\nTrainable params: 20,123,648\nNon-trainable params: 7,636,800\n_________________________________________________________________\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "class TripletModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TripletModel, self).__init__()\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def call(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        anchor_embedding = self.embedding(anchor)\n",
    "        positive_embedding = self.embedding(positive)\n",
    "        negative_embedding = self.embedding(negative)\n",
    "        return anchor_embedding, positive_embedding, negative_embedding"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.583562Z",
     "iopub.execute_input": "2023-07-08T13:44:05.583901Z",
     "iopub.status.idle": "2023-07-08T13:44:05.598852Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.583870Z",
     "shell.execute_reply": "2023-07-08T13:44:05.598031Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cosine_similarity_batched(x1, x2):\n",
    "    # Compute L2-norm of each vector\n",
    "    x1_norm = tf.linalg.norm(x1, axis=1, keepdims=True)\n",
    "    x2_norm = tf.linalg.norm(x2, axis=1, keepdims=True)\n",
    "    # Compute dot product between vectors\n",
    "    dot_product = tf.reduce_sum(x1 * x2, axis=1, keepdims=True)\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (x1_norm * x2_norm)\n",
    "\n",
    "    return cosine_similarity"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.599915Z",
     "iopub.execute_input": "2023-07-08T13:44:05.600277Z",
     "iopub.status.idle": "2023-07-08T13:44:05.611273Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.600247Z",
     "shell.execute_reply": "2023-07-08T13:44:05.610578Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def custom_f1_score_pos(y_true, y_pred):\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    TP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), dtype=tf.float32), axis=0)\n",
    "    FP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), dtype=tf.float32), axis=0)\n",
    "    FN = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), dtype=tf.float32), axis=0)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.612486Z",
     "iopub.execute_input": "2023-07-08T13:44:05.612814Z",
     "iopub.status.idle": "2023-07-08T13:44:05.625567Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.612784Z",
     "shell.execute_reply": "2023-07-08T13:44:05.624624Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def custom_f1_score_neg(y_true, y_pred):\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    TP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 0)), dtype=tf.float32), axis=0)\n",
    "    FP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), dtype=tf.float32), axis=0)\n",
    "    FN = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), dtype=tf.float32), axis=0)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.626838Z",
     "iopub.execute_input": "2023-07-08T13:44:05.627193Z",
     "iopub.status.idle": "2023-07-08T13:44:05.637032Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.627165Z",
     "shell.execute_reply": "2023-07-08T13:44:05.636028Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size=32, image_size=(224, 224)):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_df = self.dataframe[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        batch_images1 = []\n",
    "        batch_images2 = []\n",
    "        batch_images3 = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            img1_url, img2_url, img3_url = row['image_url1'], row['image_url2'], row['image_url3']\n",
    "            image1 = Image.open(os.path.join(dir_img_1, img1_url))\n",
    "            image2 = Image.open(os.path.join(dir_img_2, img2_url))\n",
    "            image3 = 0\n",
    "            try:\n",
    "                image3 = Image.open(os.path.join(dir_img_1, img3_url))\n",
    "            except FileNotFoundError:\n",
    "                image3 = Image.open(os.path.join(dir_img_2, img3_url))\n",
    "                \n",
    "                \n",
    "            if image1.mode == \"L\" or image1.mode == \"RGBA\":\n",
    "                image1 = image1.convert(\"RGB\")\n",
    "            if image2.mode == \"L\" or image2.mode == \"RGBA\":\n",
    "                image2 = image2.convert(\"RGB\")\n",
    "            if image3.mode == \"L\" or image3.mode == \"RGBA\":\n",
    "                image3 = image3.convert(\"RGB\")\n",
    "            \n",
    "            image1 = image1.resize((224, 224))  \n",
    "            image2 = image2.resize((224, 224)) \n",
    "            image3 = image3.resize((224, 224))\n",
    "                \n",
    "            image1 = tf.convert_to_tensor(image1)\n",
    "            image2 = tf.convert_to_tensor(image2)\n",
    "            image3 = tf.convert_to_tensor(image3)\n",
    "\n",
    "            batch_images1.append(image1)\n",
    "            batch_images2.append(image2)\n",
    "            batch_images3.append(image3)\n",
    "\n",
    "        batch_images1 = np.array(batch_images1) / 255.0\n",
    "        batch_images2 = np.array(batch_images2) / 255.0\n",
    "        batch_images3 = np.array(batch_images3) /  255.0\n",
    "        return batch_images1, batch_images2, batch_images3\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.dataframe = self.dataframe.sample(frac=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.638526Z",
     "iopub.execute_input": "2023-07-08T13:44:05.639210Z",
     "iopub.status.idle": "2023-07-08T13:44:05.661671Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.639146Z",
     "shell.execute_reply": "2023-07-08T13:44:05.660680Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "threshold = 0.77"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.663187Z",
     "iopub.execute_input": "2023-07-08T13:44:05.663834Z",
     "iopub.status.idle": "2023-07-08T13:44:05.670693Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.663786Z",
     "shell.execute_reply": "2023-07-08T13:44:05.669729Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def custom_loss(labels, predictions):\n",
    "    # Compute cosine distances between anchor, positive, and negative examples\n",
    "    anchor_embedding, positive_embedding, negative_embedding = predictions\n",
    "    pos_sim = cosine_similarity_batched(anchor_embedding, positive_embedding)\n",
    "    neg_sim = cosine_similarity_batched(anchor_embedding, negative_embedding)\n",
    "    pos_dist = 1 - pos_sim\n",
    "    neg_dist = 1 - neg_sim\n",
    "    pos_f1 =  custom_f1_score_pos(tf.expand_dims(labels[1], axis=1), tf.cast(pos_sim  > threshold, tf.int32))\n",
    "    neg_f1 = custom_f1_score_neg(tf.expand_dims(labels[0], axis=1), tf.cast(neg_sim  > threshold, tf.int32))\n",
    "    alpha = 1\n",
    "    return K.mean(K.maximum(pos_dist - neg_dist + alpha, 0.0)), (pos_f1 + neg_f1)/2"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.672072Z",
     "iopub.execute_input": "2023-07-08T13:44:05.673164Z",
     "iopub.status.idle": "2023-07-08T13:44:05.681815Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.673132Z",
     "shell.execute_reply": "2023-07-08T13:44:05.680625Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "weights = []\n",
    "best_metric = np.inf"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.683219Z",
     "iopub.execute_input": "2023-07-08T13:44:05.684221Z",
     "iopub.status.idle": "2023-07-08T13:44:05.691939Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.684189Z",
     "shell.execute_reply": "2023-07-08T13:44:05.690584Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, train_dataset, val_dataset):\n",
    "    # Define loss function, optimizer, and metrics\n",
    "    loss_fn = custom_loss\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    train_loss_metric = 0.0\n",
    "    val_loss_metric = 0.0\n",
    "    train_f1_metric =  tf.keras.metrics.Mean()\n",
    "    val_f1_metric =  tf.keras.metrics.Mean()\n",
    "    epochs = 10\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x1, x2, x3):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = model([x1, x2, x3])\n",
    "            labels = [j for j in [0,1] for i in range(x1.shape[0])]\n",
    "            labels = tf.constant(labels)\n",
    "            labels = tf.reshape(labels, (2,x1.shape[0]))\n",
    "            loss_value, train_f1 = loss_fn(labels, predictions)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss_value, train_f1\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(x1, x2, x3):\n",
    "        # Forward pass\n",
    "        predictions = model([x1, x2, x3])\n",
    "        labels = [j for j in [0,1] for i in range(x1.shape[0])]\n",
    "        labels = tf.constant(labels)\n",
    "        labels = tf.reshape(labels, (2,x1.shape[0]))\n",
    "        loss_value, val_f1 = loss_fn(labels, predictions)\n",
    "        return loss_value, val_f1\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Reset metrics\n",
    "        train_loss_metric = 0.0\n",
    "        val_loss_metric = 0.0\n",
    "        train_f1_metric.reset_states()\n",
    "        val_f1_metric.reset_states()\n",
    "        train_k = 0\n",
    "        val_k = 0\n",
    "\n",
    "        # Training\n",
    "        for x1, x2, x3 in train_dataset:\n",
    "            loss_value, train_f1 = train_step(x1, x2, x3)\n",
    "            train_loss_metric += loss_value\n",
    "            train_k += 1\n",
    "            train_f1_metric.update_state(train_f1)\n",
    "        \n",
    "        # Validation\n",
    "        for x1, x2, x3 in val_dataset:\n",
    "            loss_value, val_f1 = val_step(x1, x2, x3)\n",
    "            val_loss_metric += loss_value\n",
    "            val_k += 1\n",
    "            val_f1_metric.update_state(val_f1)\n",
    "            \n",
    "        train_loss_metric /= (train_k)\n",
    "        val_loss_metric /= (val_k)\n",
    "        \n",
    "        # Record the epoch losses\n",
    "        train_losses.append(train_loss_metric)\n",
    "        val_losses.append(val_loss_metric)\n",
    "        \n",
    "        global best_metric\n",
    "        \n",
    "        if val_loss_metric < best_metric:\n",
    "            best_metric = val_loss_metric\n",
    "            model.save('/kaggle/working/best_model', save_format='tf')\n",
    "            global weights\n",
    "            weights = model.layers[0].get_weights()  \n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss_metric}, Train F1: {train_f1_metric.result().numpy()}\")\n",
    "        print(f\"Val Loss: {val_loss_metric}, Val F1: {val_f1_metric.result().numpy()}\")\n",
    "        print()\n",
    "\n",
    "    return train_losses, val_losses"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.693785Z",
     "iopub.execute_input": "2023-07-08T13:44:05.694589Z",
     "iopub.status.idle": "2023-07-08T13:44:05.713659Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.694558Z",
     "shell.execute_reply": "2023-07-08T13:44:05.712510Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # detect and init the TPU\n",
    "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    "#  # instantiate a distribution strategy\n",
    "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.717141Z",
     "iopub.execute_input": "2023-07-08T13:44:05.717498Z",
     "iopub.status.idle": "2023-07-08T13:44:05.726771Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.717450Z",
     "shell.execute_reply": "2023-07-08T13:44:05.725910Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = TripletModel()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.728065Z",
     "iopub.execute_input": "2023-07-08T13:44:05.729187Z",
     "iopub.status.idle": "2023-07-08T13:44:05.740627Z",
     "shell.execute_reply.started": "2023-07-08T13:44:05.729136Z",
     "shell.execute_reply": "2023-07-08T13:44:05.739850Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 2\n",
    "\n",
    "# Initialize the K-Fold object\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store the training histories for each fold\n",
    "train_histories = []\n",
    "val_histories = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in kf.split(train_data):\n",
    "    train_dataframe = train_data.iloc[train_index]\n",
    "    val_dataframe = train_data.iloc[val_index]\n",
    "\n",
    "    train_data_generator = DataGenerator(train_dataframe, batch_size=32)\n",
    "    val_data_generator = DataGenerator(val_dataframe, batch_size=32)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: train_data_generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "             tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: val_data_generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "             tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "           tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    # Perform the custom model training\n",
    "    train_losses, val_losses = train_model(model, train_dataset, val_dataset)\n",
    "\n",
    "    # Store the training histories for each fold\n",
    "    train_histories.append(train_losses)\n",
    "    val_histories.append(val_losses)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-08T13:44:05.741818Z",
     "iopub.execute_input": "2023-07-08T13:44:05.742811Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/10:\nTrain Loss: 0.21746133267879486, Train F1: nan\nVal Loss: 0.18608200550079346, Val F1: 0.9958697557449341\n\nEpoch 2/10:\nTrain Loss: 0.059792980551719666, Train F1: 0.9954248666763306\nVal Loss: 0.08521177619695663, Val F1: 0.9913681149482727\n\nEpoch 3/10:\nTrain Loss: 0.020478516817092896, Train F1: 0.9976146817207336\nVal Loss: 0.07740872353315353, Val F1: 0.9913020133972168\n\nEpoch 4/10:\nTrain Loss: 0.006604009307920933, Train F1: 0.9986353516578674\nVal Loss: 0.0808311179280281, Val F1: 0.9917841553688049\n\nEpoch 5/10:\nTrain Loss: 0.002744462573900819, Train F1: 0.9988847374916077\nVal Loss: 0.07847895473241806, Val F1: 0.99183189868927\n\nEpoch 6/10:\nTrain Loss: 0.0012684103567153215, Train F1: 0.9993935227394104\nVal Loss: 0.07602201402187347, Val F1: 0.9920370578765869\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(weights)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training results for each fold\n",
    "for fold, (train_losses, val_losses) in enumerate(zip(train_histories, val_histories)):\n",
    "    # Plot loss curves\n",
    "    plt.plot(train_losses, label=f'Train ')\n",
    "    plt.plot(val_losses, label=f'Validation ')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
