{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-02T16:00:53.925438Z",
     "iopub.status.busy": "2023-07-02T16:00:53.924598Z",
     "iopub.status.idle": "2023-07-02T16:02:42.446014Z",
     "shell.execute_reply": "2023-07-02T16:02:42.444964Z",
     "shell.execute_reply.started": "2023-07-02T16:00:53.925403Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        os.path.join(dirname, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:55:28.854062Z",
     "iopub.status.busy": "2023-07-03T05:55:28.853672Z",
     "iopub.status.idle": "2023-07-03T05:55:39.060486Z",
     "shell.execute_reply": "2023-07-03T05:55:39.059497Z",
     "shell.execute_reply.started": "2023-07-03T05:55:28.854032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from scipy.spatial import cKDTree\n",
    "from skimage.feature import plot_matches\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import AffineTransform\n",
    "from six import BytesIO\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from six.moves.urllib.request import urlopen\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:55:15.977582Z",
     "iopub.status.busy": "2023-07-03T05:55:15.976455Z",
     "iopub.status.idle": "2023-07-03T05:55:15.996475Z",
     "shell.execute_reply": "2023-07-03T05:55:15.995430Z",
     "shell.execute_reply.started": "2023-07-03T05:55:15.977533Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:55:18.596499Z",
     "iopub.status.busy": "2023-07-03T05:55:18.595887Z",
     "iopub.status.idle": "2023-07-03T05:55:18.793402Z",
     "shell.execute_reply": "2023-07-03T05:55:18.792351Z",
     "shell.execute_reply.started": "2023-07-03T05:55:18.596440Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/new-data-lun-csv/new_data_lun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:55:20.787781Z",
     "iopub.status.busy": "2023-07-03T05:55:20.787297Z",
     "iopub.status.idle": "2023-07-03T05:55:20.793331Z",
     "shell.execute_reply": "2023-07-03T05:55:20.792212Z",
     "shell.execute_reply.started": "2023-07-03T05:55:20.787747Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_img_1 = '/kaggle/input/traindataset/LUN_DataSet/train_url1'\n",
    "dir_img_2 = '/kaggle/input/traindataset/LUN_DataSet/train_url2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:55:39.062997Z",
     "iopub.status.busy": "2023-07-03T05:55:39.062239Z",
     "iopub.status.idle": "2023-07-03T05:55:44.952591Z",
     "shell.execute_reply": "2023-07-03T05:55:44.951579Z",
     "shell.execute_reply.started": "2023-07-03T05:55:39.062938Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_vect_mob_net = hub.KerasLayer('https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T06:28:45.399797Z",
     "iopub.status.busy": "2023-07-03T06:28:45.399408Z",
     "iopub.status.idle": "2023-07-03T06:28:45.409954Z",
     "shell.execute_reply": "2023-07-03T06:28:45.408623Z",
     "shell.execute_reply.started": "2023-07-03T06:28:45.399766Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_batched(x1, x2):\n",
    "    # Compute L2-norm of each vector\n",
    "    x1_norm = tf.linalg.norm(x1, axis=1, keepdims=True)\n",
    "    x2_norm = tf.linalg.norm(x2, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute dot product between vectors\n",
    "    dot_product = tf.reduce_sum(x1 * x2, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (x1_norm * x2_norm + tf.keras.backend.epsilon())\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:55:44.956769Z",
     "iopub.status.busy": "2023-07-03T05:55:44.953950Z",
     "iopub.status.idle": "2023-07-03T05:55:44.964740Z",
     "shell.execute_reply": "2023-07-03T05:55:44.963854Z",
     "shell.execute_reply.started": "2023-07-03T05:55:44.956739Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_f1_score(y_true, y_pred):\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    TP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), dtype=tf.float32), axis=0)\n",
    "    FP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), dtype=tf.float32), axis=0)\n",
    "    FN = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), dtype=tf.float32), axis=0)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP + tf.keras.backend.epsilon())\n",
    "    recall = TP / (TP + FN + tf.keras.backend.epsilon())\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T06:28:53.828947Z",
     "iopub.status.busy": "2023-07-03T06:28:53.828072Z",
     "iopub.status.idle": "2023-07-03T06:28:53.835577Z",
     "shell.execute_reply": "2023-07-03T06:28:53.834657Z",
     "shell.execute_reply.started": "2023-07-03T06:28:53.828904Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def create_siamese_model(input_shape):\n",
    "    # Create base models\n",
    "    base_model_1 = feat_vect_mob_net\n",
    "    base_model_2 = feat_vect_mob_net\n",
    "\n",
    "    # Define inputs\n",
    "    input_1 = tf.keras.Input(shape=input_shape)\n",
    "    input_2 = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Get embeddings from base models\n",
    "    embedding_1 = base_model_1(input_1)\n",
    "    embedding_2 = base_model_2(input_2)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = cosine_similarity_batched(embedding_1, embedding_2)\n",
    "\n",
    "    # Create siamese model\n",
    "    siamese_model = tf.keras.Model(inputs=[input_1, input_2], outputs=cosine_similarity)\n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:57:52.028889Z",
     "iopub.status.busy": "2023-07-03T05:57:52.028521Z",
     "iopub.status.idle": "2023-07-03T05:57:52.042640Z",
     "shell.execute_reply": "2023-07-03T05:57:52.041310Z",
     "shell.execute_reply.started": "2023-07-03T05:57:52.028858Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size=32, image_size=(224, 224)):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_df = self.dataframe[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        batch_images1 = []\n",
    "        batch_images2 = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            img1_url, img2_url, label = row['image_url1'], row['image_url2'], row['is_same']\n",
    "            image1 = Image.open(os.path.join(dir_img_1, img1_url))\n",
    "            image2 = Image.open(os.path.join(dir_img_2, img2_url))\n",
    "            if image1.mode == \"L\" or image1.mode == \"RGBA\":\n",
    "                image1 = image1.convert(\"RGB\")\n",
    "            if image2.mode == \"L\" or image2.mode == \"RGBA\":\n",
    "                image2 = image2.convert(\"RGB\")\n",
    "            \n",
    "            image1 = image1.resize((224, 224))  \n",
    "            image2 = image2.resize((224, 224))  \n",
    "                \n",
    "            image1 = tf.convert_to_tensor(image1)\n",
    "            image2 = tf.convert_to_tensor(image2)\n",
    "\n",
    "            batch_images1.append(image1)\n",
    "            batch_images2.append(image2)\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        batch_images1 = np.array(batch_images1) / 255.0  # Normalize images\n",
    "        batch_images2 = np.array(batch_images2) / 255.0  # Normalize images\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        return batch_images1, batch_images2, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.dataframe = self.dataframe.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:56:04.047165Z",
     "iopub.status.busy": "2023-07-03T05:56:04.046780Z",
     "iopub.status.idle": "2023-07-03T05:56:04.054824Z",
     "shell.execute_reply": "2023-07-03T05:56:04.053888Z",
     "shell.execute_reply.started": "2023-07-03T05:56:04.047135Z"
    }
   },
   "outputs": [],
   "source": [
    "split_index = int(len(train_data)*0.8)\n",
    "train_dataframe = train_data[:split_index]\n",
    "val_dataframe = train_data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T05:56:05.787636Z",
     "iopub.status.busy": "2023-07-03T05:56:05.786841Z",
     "iopub.status.idle": "2023-07-03T05:56:05.870749Z",
     "shell.execute_reply": "2023-07-03T05:56:05.869783Z",
     "shell.execute_reply.started": "2023-07-03T05:56:05.787600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(train_dataframe, batch_size=32)\n",
    "val_data_generator = DataGenerator(val_dataframe, batch_size=32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: val_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T06:32:07.599751Z",
     "iopub.status.busy": "2023-07-03T06:32:07.599317Z",
     "iopub.status.idle": "2023-07-03T06:32:07.604682Z",
     "shell.execute_reply": "2023-07-03T06:32:07.603545Z",
     "shell.execute_reply.started": "2023-07-03T06:32:07.599697Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-03T06:42:50.918844Z",
     "iopub.status.busy": "2023-07-03T06:42:50.918406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 0.9577118754386902, Train F1: 0.6902894377708435\n",
      "Val Loss: 0.87056964635849, Val F1: 0.7065821290016174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom training method\n",
    "def train_model(model, train_dataset, val_dataset, threshold):\n",
    "    # Define loss function\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    # Define metrics\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    train_f1 = tf.keras.metrics.Mean()\n",
    "    \n",
    "    best_metric = np.inf\n",
    "\n",
    "    val_loss = tf.keras.metrics.Mean()\n",
    "    val_f1 = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x1, x2, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = model([x1, x2])\n",
    "            labels = tf.expand_dims(labels, axis=1)\n",
    "            loss_value = loss_fn(labels, predictions)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1_value = custom_f1_score(labels, tf.cast(predictions > threshold, tf.int32))\n",
    "\n",
    "        return loss_value, f1_value\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(x1, x2, labels):\n",
    "        # Forward pass\n",
    "        predictions = model([x1, x2])\n",
    "        labels = tf.expand_dims(labels, axis=1)\n",
    "        loss_value = loss_fn(labels, predictions)\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1_value = custom_f1_score(labels, tf.cast(predictions > threshold, tf.int32))\n",
    "\n",
    "        return loss_value, f1_value\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 15\n",
    "    for epoch in range(epochs):\n",
    "        # Reset metrics\n",
    "        train_loss.reset_states()\n",
    "        train_f1.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_f1.reset_states()\n",
    "\n",
    "        # Training\n",
    "        for x1, x2, labels in train_dataset:\n",
    "            loss_value, f1_value = train_step(x1, x2, labels)\n",
    "            train_loss.update_state(loss_value)\n",
    "            train_f1.update_state(f1_value)\n",
    "\n",
    "        # Validation\n",
    "        for x1, x2, labels in val_dataset:\n",
    "            loss_value, f1_value = val_step(x1, x2, labels)\n",
    "            val_loss.update_state(loss_value)\n",
    "            val_f1.update_state(f1_value)\n",
    "\n",
    "        if val_loss.result() < best_metric:\n",
    "            best_metric = val_loss.result()\n",
    "            model.save('/kaggle/working/best_model.h5')\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss.result()}, Train F1: {train_f1.result()}\")\n",
    "        print(f\"Val Loss: {val_loss.result()}, Val F1: {val_f1.result()}\")\n",
    "        print()\n",
    "\n",
    "# Example usage\n",
    "input_shape = (224, 224, 3)\n",
    "threshold = 0.75\n",
    "\n",
    "# Create the model\n",
    "model = create_siamese_model(input_shape)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dataset, val_dataset, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
