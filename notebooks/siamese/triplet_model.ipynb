{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:51.785116Z",
     "iopub.status.busy": "2023-07-06T19:39:51.784214Z",
     "iopub.status.idle": "2023-07-06T19:39:51.879227Z",
     "shell.execute_reply": "2023-07-06T19:39:51.877757Z",
     "shell.execute_reply.started": "2023-07-06T19:39:51.785072Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('/kaggle/input/new-data-lun-csv/new_data_lun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:51.881439Z",
     "iopub.status.busy": "2023-07-06T19:39:51.881115Z",
     "iopub.status.idle": "2023-07-06T19:39:51.899314Z",
     "shell.execute_reply": "2023-07-06T19:39:51.898168Z",
     "shell.execute_reply.started": "2023-07-06T19:39:51.881412Z"
    }
   },
   "outputs": [],
   "source": [
    "same = train_data[train_data[\"is_same\"]==1]\n",
    "different = train_data[train_data[\"is_same\"]==0]\n",
    "new_train_data = pd.concat([same, different])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:51.900724Z",
     "iopub.status.busy": "2023-07-06T19:39:51.900429Z",
     "iopub.status.idle": "2023-07-06T19:39:51.923665Z",
     "shell.execute_reply": "2023-07-06T19:39:51.922683Z",
     "shell.execute_reply.started": "2023-07-06T19:39:51.900699Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train_data[\"image_url3\"] = list(new_train_data[\"image_url1\"].values)[::-1]\n",
    "new_train_data = new_train_data[new_train_data[\"is_same\"]==1]\n",
    "new_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:51.926438Z",
     "iopub.status.busy": "2023-07-06T19:39:51.926080Z",
     "iopub.status.idle": "2023-07-06T19:39:51.931651Z",
     "shell.execute_reply": "2023-07-06T19:39:51.930640Z",
     "shell.execute_reply.started": "2023-07-06T19:39:51.926407Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from six import BytesIO\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:51.933163Z",
     "iopub.status.busy": "2023-07-06T19:39:51.932839Z",
     "iopub.status.idle": "2023-07-06T19:39:51.940243Z",
     "shell.execute_reply": "2023-07-06T19:39:51.939356Z",
     "shell.execute_reply.started": "2023-07-06T19:39:51.933137Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:51.941487Z",
     "iopub.status.busy": "2023-07-06T19:39:51.941227Z",
     "iopub.status.idle": "2023-07-06T19:39:51.948180Z",
     "shell.execute_reply": "2023-07-06T19:39:51.947205Z",
     "shell.execute_reply.started": "2023-07-06T19:39:51.941456Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_img_1 = '/kaggle/input/traindataset/LUN_DataSet/train_url1'\n",
    "dir_img_2 = '/kaggle/input/traindataset/LUN_DataSet/train_url2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:51.949782Z",
     "iopub.status.busy": "2023-07-06T19:39:51.949335Z",
     "iopub.status.idle": "2023-07-06T19:39:53.959279Z",
     "shell.execute_reply": "2023-07-06T19:39:53.957969Z",
     "shell.execute_reply.started": "2023-07-06T19:39:51.949757Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_ext = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:47:00.993597Z",
     "iopub.status.busy": "2023-07-06T19:47:00.993042Z",
     "iopub.status.idle": "2023-07-06T19:47:01.002453Z",
     "shell.execute_reply": "2023-07-06T19:47:01.001123Z",
     "shell.execute_reply.started": "2023-07-06T19:47:00.993545Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "class TripletModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TripletModel, self).__init__()\n",
    "        self.feat_ext = feat_ext\n",
    "\n",
    "    def call(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        anchor_embedding = self.feat_ext(anchor)\n",
    "        positive_embedding = self.feat_ext(positive)\n",
    "        negative_embedding = self.feat_ext(negative)\n",
    "        return anchor_embedding, positive_embedding, negative_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T20:00:04.904686Z",
     "iopub.status.busy": "2023-07-06T20:00:04.904212Z",
     "iopub.status.idle": "2023-07-06T20:00:04.911744Z",
     "shell.execute_reply": "2023-07-06T20:00:04.910605Z",
     "shell.execute_reply.started": "2023-07-06T20:00:04.904652Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_batched(x1, x2):\n",
    "    # Compute L2-norm of each vector\n",
    "    x1_norm = tf.linalg.norm(x1, axis=1, keepdims=True)\n",
    "    x2_norm = tf.linalg.norm(x2, axis=1, keepdims=True)\n",
    "    # Compute dot product between vectors\n",
    "    dot_product = tf.reduce_sum(x1 * x2, axis=1, keepdims=True)\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (x1_norm * x2_norm)\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:54.013723Z",
     "iopub.status.busy": "2023-07-06T19:39:54.013417Z",
     "iopub.status.idle": "2023-07-06T19:39:54.023057Z",
     "shell.execute_reply": "2023-07-06T19:39:54.022149Z",
     "shell.execute_reply.started": "2023-07-06T19:39:54.013699Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_f1_score(y_true, y_pred):\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    TP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), dtype=tf.float32), axis=0)\n",
    "    FP = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), dtype=tf.float32), axis=0)\n",
    "    FN = tf.math.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), dtype=tf.float32), axis=0)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP + tf.keras.backend.epsilon())\n",
    "    recall = TP / (TP + FN + tf.keras.backend.epsilon())\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:54.036487Z",
     "iopub.status.busy": "2023-07-06T19:39:54.036156Z",
     "iopub.status.idle": "2023-07-06T19:39:54.054634Z",
     "shell.execute_reply": "2023-07-06T19:39:54.053624Z",
     "shell.execute_reply.started": "2023-07-06T19:39:54.036461Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size=32, image_size=(224, 224)):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_df = self.dataframe[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        batch_images1 = []\n",
    "        batch_images2 = []\n",
    "        batch_images3 = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            img1_url, img2_url, img3_url = row['image_url1'], row['image_url2'], row['image_url3']\n",
    "            image1 = Image.open(os.path.join(dir_img_1, img1_url))\n",
    "            image2 = Image.open(os.path.join(dir_img_2, img2_url))\n",
    "            image3 = 0\n",
    "            try:\n",
    "                image3 = Image.open(os.path.join(dir_img_1, img3_url))\n",
    "            except FileNotFoundError:\n",
    "                image3 = Image.open(os.path.join(dir_img_2, img3_url))\n",
    "                \n",
    "                \n",
    "            if image1.mode == \"L\" or image1.mode == \"RGBA\":\n",
    "                image1 = image1.convert(\"RGB\")\n",
    "            if image2.mode == \"L\" or image2.mode == \"RGBA\":\n",
    "                image2 = image2.convert(\"RGB\")\n",
    "            if image3.mode == \"L\" or image3.mode == \"RGBA\":\n",
    "                image3 = image3.convert(\"RGB\")\n",
    "            \n",
    "            image1 = image1.resize((224, 224))  \n",
    "            image2 = image2.resize((224, 224)) \n",
    "            image3 = image3.resize((224, 224))\n",
    "                \n",
    "            image1 = tf.convert_to_tensor(image1)\n",
    "            image2 = tf.convert_to_tensor(image2)\n",
    "            image3 = tf.convert_to_tensor(image3)\n",
    "\n",
    "            batch_images1.append(image1)\n",
    "            batch_images2.append(image2)\n",
    "            batch_images3.append(image3)\n",
    "\n",
    "        batch_images1 = np.array(batch_images1) / 255.0\n",
    "        batch_images2 = np.array(batch_images2) / 255.0\n",
    "        batch_images3 = np.array(batch_images3) /  255.0\n",
    "        return batch_images1, batch_images2, batch_images3\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.dataframe = self.dataframe.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T20:04:09.549163Z",
     "iopub.status.busy": "2023-07-06T20:04:09.548302Z",
     "iopub.status.idle": "2023-07-06T20:04:09.555622Z",
     "shell.execute_reply": "2023-07-06T20:04:09.554555Z",
     "shell.execute_reply.started": "2023-07-06T20:04:09.549127Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_loss(labels, predictions):\n",
    "    # Compute cosine distances between anchor, positive, and negative examples\n",
    "    anchor_embedding, positive_embedding, negative_embedding = predictions\n",
    "    pos_sim = cosine_similarity_batched(anchor_embedding, positive_embedding)\n",
    "    neg_sim = cosine_similarity_batched(anchor_embedding, negative_embedding)\n",
    "    pos_dist = 1 - pos_sim\n",
    "    neg_dist = 1 - neg_sim\n",
    "    pos_f1 = custom_f1_score(tf.expand_dims(labels[1], axis=1), tf.cast(pos_sim  > 0.75, tf.int32))\n",
    "    neg_f1 = custom_f1_score(tf.expand_dims(labels[0], axis=1), tf.cast(neg_sim  > 0.75, tf.int32))\n",
    "    alpha = 1\n",
    "    return K.mean(K.maximum(pos_dist - neg_dist + alpha, 0.0)), (pos_f1 + neg_f1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:39:54.141587Z",
     "iopub.status.busy": "2023-07-06T19:39:54.141317Z",
     "iopub.status.idle": "2023-07-06T19:39:54.149511Z",
     "shell.execute_reply": "2023-07-06T19:39:54.148612Z",
     "shell.execute_reply.started": "2023-07-06T19:39:54.141564Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "best_metric = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset):\n",
    "    # Define loss function, optimizer, and metrics\n",
    "    loss_fn = custom_loss\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    train_loss_metric = 0.0\n",
    "    val_loss_metric = 0.0\n",
    "    train_f1_metric =  tf.keras.metrics.Mean()\n",
    "    val_f1_metric =  tf.keras.metrics.Mean()\n",
    "    epochs = 10\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x1, x2, x3):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = model([x1, x2, x3])\n",
    "            labels = [j for j in [0,1] for i in range(x1.shape[0])]\n",
    "            labels = tf.constant(labels)\n",
    "            labels = tf.reshape(labels, (2,x1.shape[0]))\n",
    "            loss_value, train_f1 = loss_fn(labels, predictions)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss_value, train_f1\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(x1, x2, x3):\n",
    "        # Forward pass\n",
    "        predictions = model([x1, x2, x3])\n",
    "        labels = [j for j in [0,1] for i in range(x1.shape[0])]\n",
    "        labels = tf.constant(labels)\n",
    "        labels = tf.reshape(labels, (2,x1.shape[0]))\n",
    "        loss_value, val_f1 = loss_fn(labels, predictions)\n",
    "        return loss_value, val_f1\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Reset metrics\n",
    "        train_loss_metric = 0.0\n",
    "        val_loss_metric = 0.0\n",
    "        train_f1_metric.reset_states()\n",
    "        val_f1_metric.reset_states()\n",
    "        train_k = 0\n",
    "        val_k = 0\n",
    "\n",
    "        # Training\n",
    "        for x1, x2, x3 in train_dataset:\n",
    "            loss_value, train_f1 = train_step(x1, x2, x3)\n",
    "            train_loss_metric += loss_value\n",
    "            train_k += 1\n",
    "            train_f1_metric.update_state(train_f1)\n",
    "        \n",
    "        # Validation\n",
    "        for x1, x2, x3 in val_dataset:\n",
    "            loss_value, val_f1 = val_step(x1, x2, x3)\n",
    "            val_loss_metric += loss_value\n",
    "            val_k += 1\n",
    "            val_f1_metric.update_state(val_f1)\n",
    "            \n",
    "        train_loss_metric /= (train_k)\n",
    "        val_loss_metric /= (val_k)\n",
    "        \n",
    "        # Record the epoch losses\n",
    "        train_losses.append(train_loss_metric)\n",
    "        val_losses.append(val_loss_metric)\n",
    "        \n",
    "        global best_metric\n",
    "        \n",
    "        if val_loss_metric < best_metric:\n",
    "            best_metric = val_loss_metric\n",
    "            model.save('/kaggle/working/best_model', save_format='tf')\n",
    "            global weights\n",
    "            weights = model.layers[0].get_weights()  \n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss_metric}, Train F1: {train_f1_metric.result().numpy()}\")\n",
    "        print(f\"Val Loss: {val_loss_metric}, Val F1: {val_f1_metric.result().numpy()}\")\n",
    "        print()\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T19:59:27.331639Z",
     "iopub.status.busy": "2023-07-06T19:59:27.331158Z",
     "iopub.status.idle": "2023-07-06T19:59:32.832557Z",
     "shell.execute_reply": "2023-07-06T19:59:32.831220Z",
     "shell.execute_reply.started": "2023-07-06T19:59:27.331603Z"
    }
   },
   "outputs": [],
   "source": [
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    " # instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T20:19:39.017113Z",
     "iopub.status.busy": "2023-07-06T20:19:39.016364Z",
     "iopub.status.idle": "2023-07-06T20:19:39.035010Z",
     "shell.execute_reply": "2023-07-06T20:19:39.033965Z",
     "shell.execute_reply.started": "2023-07-06T20:19:39.017078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "with tpu_strategy.scope():\n",
    "    model = TripletModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T20:19:40.664891Z",
     "iopub.status.busy": "2023-07-06T20:19:40.664061Z",
     "iopub.status.idle": "2023-07-06T20:19:56.550470Z",
     "shell.execute_reply": "2023-07-06T20:19:56.548970Z",
     "shell.execute_reply.started": "2023-07-06T20:19:40.664859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 3\n",
    "\n",
    "# Initialize the K-Fold object\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store the training histories for each fold\n",
    "train_histories = []\n",
    "val_histories = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in kf.split(train_data):\n",
    "    train_dataframe = train_data.iloc[train_index]\n",
    "    val_dataframe = train_data.iloc[val_index]\n",
    "\n",
    "    train_data_generator = DataGenerator(train_dataframe, batch_size=32)\n",
    "    val_data_generator = DataGenerator(val_dataframe, batch_size=32)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: train_data_generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "             tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: val_data_generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "             tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "           tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    # Perform the custom model training\n",
    "    train_losses, val_losses = train_model(model, train_dataset, val_dataset)\n",
    "\n",
    "    # Store the training histories for each fold\n",
    "    train_histories.append(train_losses)\n",
    "    val_histories.append(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-06T19:40:03.565158Z",
     "iopub.status.idle": "2023-07-06T19:40:03.565537Z",
     "shell.execute_reply": "2023-07-06T19:40:03.565363Z",
     "shell.execute_reply.started": "2023-07-06T19:40:03.565346Z"
    }
   },
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training results for each fold\n",
    "for fold, (train_losses, val_losses) in enumerate(zip(train_histories, val_histories)):\n",
    "    # Plot loss curves\n",
    "    plt.plot(train_losses, label=f'Train (Fold {fold+1})')\n",
    "    plt.plot(val_losses, label=f'Validation (Fold {fold+1})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
